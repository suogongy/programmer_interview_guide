# äººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹  - æ€»è§ˆ

## æ¦‚è¿°

äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ å·²æˆä¸ºå½“ä»Šæœ€çƒ­é—¨çš„æŠ€æœ¯é¢†åŸŸï¼Œæ·±åˆ»æ”¹å˜ç€å„è¡Œå„ä¸šã€‚æœ¬ç« èŠ‚æ¶µç›–æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºã€æ·±åº¦å­¦ä¹ æ¡†æ¶ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ä»¥åŠMLOpså·¥ç¨‹å®è·µï¼Œä¸ºAIç›¸å…³å²—ä½é¢è¯•åšå¥½å‡†å¤‡ã€‚

## ğŸ“š ç« èŠ‚å†…å®¹

### 1. [æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ](./ml-fundamentals.md)
- **ç›‘ç£å­¦ä¹ **: çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—
- **æ— ç›‘ç£å­¦ä¹ **: èšç±»ç®—æ³•ã€é™ç»´æŠ€æœ¯ã€å¼‚å¸¸æ£€æµ‹
- **å¼ºåŒ–å­¦ä¹ **: Q-Learningã€ç­–ç•¥æ¢¯åº¦ã€æ·±åº¦å¼ºåŒ–å­¦ä¹ 
- **æ¨¡å‹è¯„ä¼°**: äº¤å‰éªŒè¯ã€åå·®-æ–¹å·®æƒè¡¡ã€æ€§èƒ½æŒ‡æ ‡
- **ç‰¹å¾å·¥ç¨‹**: ç‰¹å¾é€‰æ‹©ã€ç‰¹å¾ç¼©æ”¾ã€ç‰¹å¾æ„é€ 

### 2. [æ·±åº¦å­¦ä¹ æ¡†æ¶](./deep-learning.md)
- **TensorFlow/Keras**: æ¨¡å‹æ„å»ºã€è®­ç»ƒã€éƒ¨ç½²
- **PyTorch**: åŠ¨æ€å›¾æœºåˆ¶ã€è‡ªåŠ¨æ±‚å¯¼ã€åˆ†å¸ƒå¼è®­ç»ƒ
- **ç¥ç»ç½‘ç»œåŸºç¡€**: å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€æ¿€æ´»å‡½æ•°
- **CNNå·ç§¯ç¥ç»ç½‘ç»œ**: å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹
- **RNNå¾ªç¯ç¥ç»ç½‘ç»œ**: åºåˆ—å»ºæ¨¡ã€LSTMã€GRU

### 3. [è‡ªç„¶è¯­è¨€å¤„ç†](./nlp.md)
- **æ–‡æœ¬é¢„å¤„ç†**: åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«
- **è¯å‘é‡**: Word2Vecã€GloVeã€FastText
- **è¯­è¨€æ¨¡å‹**: N-gramã€ç¥ç»è¯­è¨€æ¨¡å‹
- **Transformeræ¶æ„**: Attentionæœºåˆ¶ã€BERTã€GPT
- **åº”ç”¨åœºæ™¯**: æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æ

### 4. [è®¡ç®—æœºè§†è§‰](./computer-vision.md)
- **å›¾åƒå¤„ç†**: æ»¤æ³¢ã€è¾¹ç¼˜æ£€æµ‹ã€å½¢æ€å­¦æ“ä½œ
- **ä¼ ç»Ÿæ–¹æ³•**: SIFTã€SURFã€HOGç‰¹å¾
- **æ·±åº¦å­¦ä¹ **: CNNã€ResNetã€YOLOã€R-CNN
- **åº”ç”¨é¢†åŸŸ**: å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²
- **å®è·µé¡¹ç›®**: äººè„¸è¯†åˆ«ã€åŒ»å­¦å½±åƒåˆ†æ

### 5. [MLOpsä¸æ¨¡å‹éƒ¨ç½²](./mlops.md)
- **æ¨¡å‹ç”Ÿå‘½å‘¨æœŸ**: æ•°æ®ç®¡ç†ã€å®éªŒè·Ÿè¸ªã€ç‰ˆæœ¬æ§åˆ¶
- **æ¨¡å‹è®­ç»ƒ**: åˆ†å¸ƒå¼è®­ç»ƒã€è¶…å‚æ•°ä¼˜åŒ–ã€AutoML
- **æ¨¡å‹éƒ¨ç½²**: REST APIã€æ‰¹å¤„ç†ã€æµå¼å¤„ç†
- **ç›‘æ§ä¸ç»´æŠ¤**: æ¨¡å‹æ¼‚ç§»ã€A/Bæµ‹è¯•ã€æŒç»­å­¦ä¹ 
- **å·¥å…·é“¾**: MLflowã€Kubeflowã€TensorFlow Serving

## ğŸ¯ æŠ€æœ¯æ ˆå¯¹æ¯”

### æ·±åº¦å­¦ä¹ æ¡†æ¶å¯¹æ¯”

| æ¡†æ¶ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| **TensorFlow** | ç”Ÿäº§å°±ç»ªã€ç”Ÿæ€å®Œå–„ | å­¦ä¹ æ›²çº¿é™¡å³­ | å·¥ä¸šéƒ¨ç½²ã€å¤§è§„æ¨¡ç³»ç»Ÿ |
| **PyTorch** | åŠ¨æ€å›¾ã€æ˜“è°ƒè¯• | éƒ¨ç½²ç›¸å¯¹å¤æ‚ | ç ”ç©¶ã€åŸå‹å¼€å‘ |
| **JAX** | å‡½æ•°å¼ã€å¯å¹¶è¡Œ | ç”Ÿæ€è¾ƒæ–° | é«˜æ€§èƒ½è®¡ç®—ã€ç ”ç©¶ |
| **PaddlePaddle** | ä¸­æ–‡æ”¯æŒå¥½ | å›½é™…ç”Ÿæ€æœ‰é™ | ä¸­æ–‡NLPã€å›½å†…åœºæ™¯ |

### äº‘MLå¹³å°å¯¹æ¯”

| å¹³å° | ç‰¹è‰²æœåŠ¡ | ä¼˜åŠ¿ | é€‚ç”¨å¯¹è±¡ |
|------|---------|------|---------|
| **AWS SageMaker** | ç«¯åˆ°ç«¯MLå·¥ä½œæµ | åŠŸèƒ½å…¨é¢ã€å¯æ‰©å±•æ€§å¼º | ä¼ä¸šçº§ç”¨æˆ· |
| **Google Colab** | å…è´¹GPU/TPU | æ˜“ç”¨ã€å…è´¹é¢åº¦ | å­¦ç”Ÿã€ç ”ç©¶è€… |
| **Azure ML** | ä¼ä¸šé›†æˆ | ä¸Officeç”Ÿæ€é›†æˆ | ä¼ä¸šç”¨æˆ· |
| **é˜¿é‡Œäº‘PAI** | æœ¬åœŸåŒ–æœåŠ¡ | ä¸­æ–‡æ”¯æŒã€åˆè§„æ€§ | å›½å†…ä¼ä¸š |

## ğŸ§  æ ¸å¿ƒç®—æ³•å®ç°

### çº¿æ€§å›å½’ï¼ˆä»é›¶å®ç°ï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt

class LinearRegression:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.costs = []
    
    def fit(self, X, y):
        # åˆå§‹åŒ–å‚æ•°
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # æ¢¯åº¦ä¸‹é™
        for i in range(self.n_iterations):
            # å‰å‘ä¼ æ’­
            y_predicted = np.dot(X, self.weights) + self.bias
            
            # è®¡ç®—ä»£ä»·å‡½æ•°
            cost = (1 / (2 * n_samples)) * np.sum((y_predicted - y) ** 2)
            self.costs.append(cost)
            
            # è®¡ç®—æ¢¯åº¦
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)
            
            # æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X):
        return np.dot(X, self.weights) + self.bias
```

### ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def relu(x):
    return np.maximum(0, x)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

class NeuralNetwork:
    def __init__(self, layer_sizes):
        self.layer_sizes = layer_sizes
        self.weights = []
        self.biases = []
        
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        for i in range(len(layer_sizes) - 1):
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.1
            b = np.zeros((1, layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def forward(self, X):
        activations = [X]
        
        for i in range(len(self.weights)):
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            
            if i == len(self.weights) - 1:  # è¾“å‡ºå±‚
                a = softmax(z)
            else:  # éšè—å±‚
                a = relu(z)
            
            activations.append(a)
        
        return activations
```

## ğŸ”¥ çƒ­é—¨åº”ç”¨åœºæ™¯

### 1. æ¨èç³»ç»Ÿ

#### ååŒè¿‡æ»¤
```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class CollaborativeFiltering:
    def __init__(self):
        self.user_similarity = None
        self.item_similarity = None
        self.user_mean = None
    
    def fit(self, ratings_matrix):
        # ratings_matrix: users x items
        self.user_mean = np.mean(ratings_matrix, axis=1, keepdims=True)
        
        # è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦
        centered_ratings = ratings_matrix - self.user_mean
        self.user_similarity = cosine_similarity(centered_ratings)
        
        # è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦
        self.item_similarity = cosine_similarity(ratings_matrix.T)
    
    def predict_user_based(self, user_id, item_id, ratings_matrix, k=10):
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„kä¸ªç”¨æˆ·
        similarities = self.user_similarity[user_id]
        similar_users = np.argsort(similarities)[::-1][1:k+1]
        
        # è®¡ç®—é¢„æµ‹è¯„åˆ†
        numerator = 0
        denominator = 0
        
        for similar_user in similar_users:
            if ratings_matrix[similar_user, item_id] > 0:
                similarity = similarities[similar_user]
                rating_diff = (ratings_matrix[similar_user, item_id] - 
                             self.user_mean[similar_user])
                numerator += similarity * rating_diff
                denominator += abs(similarity)
        
        if denominator == 0:
            return self.user_mean[user_id, 0]
        
        prediction = self.user_mean[user_id, 0] + numerator / denominator
        return np.clip(prediction, 1, 5)
```

### 2. æ—¶é—´åºåˆ—é¢„æµ‹

#### LSTMæ¨¡å‹
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

def create_lstm_model(sequence_length, n_features, n_outputs):
    model = Sequential([
        LSTM(50, return_sequences=True, 
             input_shape=(sequence_length, n_features)),
        Dropout(0.2),
        
        LSTM(50, return_sequences=True),
        Dropout(0.2),
        
        LSTM(50),
        Dropout(0.2),
        
        Dense(n_outputs)
    ])
    
    model.compile(
        optimizer='adam',
        loss='mse',
        metrics=['mae']
    )
    
    return model

def prepare_time_series_data(data, sequence_length):
    X, y = [], []
    
    for i in range(len(data) - sequence_length):
        X.append(data[i:(i + sequence_length)])
        y.append(data[i + sequence_length])
    
    return np.array(X), np.array(y)
```

## ğŸ“Š æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–

### åˆ†ç±»æ¨¡å‹è¯„ä¼°
```python
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_classification_model(y_true, y_pred, class_names=None):
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()
    
    # åˆ†ç±»æŠ¥å‘Š
    print(classification_report(y_true, y_pred, target_names=class_names))
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    tp = np.diag(cm)
    fp = cm.sum(axis=0) - tp
    fn = cm.sum(axis=1) - tp
    tn = cm.sum() - (fp + fn + tp)
    
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1 = 2 * (precision * recall) / (precision + recall)
    
    return {
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'confusion_matrix': cm
    }
```

### è¶…å‚æ•°ä¼˜åŒ–
```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier

def hyperparameter_tuning_example():
    # ç½‘æ ¼æœç´¢
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    rf = RandomForestClassifier(random_state=42)
    
    # ç½‘æ ¼æœç´¢
    grid_search = GridSearchCV(
        estimator=rf,
        param_grid=param_grid,
        cv=5,
        scoring='f1_macro',
        n_jobs=-1,
        verbose=1
    )
    
    # éšæœºæœç´¢ï¼ˆæ›´é«˜æ•ˆï¼‰
    random_search = RandomizedSearchCV(
        estimator=rf,
        param_distributions=param_grid,
        n_iter=50,
        cv=5,
        scoring='f1_macro',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )
    
    return grid_search, random_search
```

## ğŸš€ MLOpsæœ€ä½³å®è·µ

### æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶
```python
import mlflow
import mlflow.sklearn
from mlflow.tracking import MlflowClient

def log_model_experiment(model, X_train, y_train, X_test, y_test, params):
    with mlflow.start_run():
        # è®°å½•è¶…å‚æ•°
        mlflow.log_params(params)
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        train_score = model.score(X_train, y_train)
        test_score = model.score(X_test, y_test)
        
        # è®°å½•æŒ‡æ ‡
        mlflow.log_metric("train_score", train_score)
        mlflow.log_metric("test_score", test_score)
        
        # è®°å½•æ¨¡å‹
        mlflow.sklearn.log_model(model, "model")
        
        # è®°å½•æ¨¡å‹ç­¾å
        signature = mlflow.models.infer_signature(X_train, model.predict(X_train))
        mlflow.sklearn.log_model(model, "model", signature=signature)
        
        return mlflow.active_run().info.run_id
```

### æ¨¡å‹ç›‘æ§
```python
import numpy as np
from scipy import stats

class ModelMonitor:
    def __init__(self, reference_data):
        self.reference_data = reference_data
        self.reference_stats = self._compute_stats(reference_data)
    
    def _compute_stats(self, data):
        return {
            'mean': np.mean(data, axis=0),
            'std': np.std(data, axis=0),
            'quantiles': np.quantile(data, [0.25, 0.5, 0.75], axis=0)
        }
    
    def detect_drift(self, new_data, threshold=0.05):
        """ä½¿ç”¨KSæ£€éªŒæ£€æµ‹æ•°æ®æ¼‚ç§»"""
        drift_results = {}
        
        for i in range(new_data.shape[1]):
            ks_stat, p_value = stats.ks_2samp(
                self.reference_data[:, i], 
                new_data[:, i]
            )
            
            drift_results[f'feature_{i}'] = {
                'ks_statistic': ks_stat,
                'p_value': p_value,
                'drift_detected': p_value < threshold
            }
        
        return drift_results
    
    def generate_report(self, new_data):
        drift_results = self.detect_drift(new_data)
        new_stats = self._compute_stats(new_data)
        
        report = {
            'drift_detection': drift_results,
            'statistical_comparison': {
                'reference_stats': self.reference_stats,
                'new_stats': new_stats
            }
        }
        
        return report
```

## ğŸ¯ é¢è¯•é‡ç‚¹

### ç†è®ºåŸºç¡€
1. **æœºå™¨å­¦ä¹ åŸºç¡€**: åå·®-æ–¹å·®æƒè¡¡ã€è¿‡æ‹Ÿåˆã€æ­£åˆ™åŒ–
2. **æ·±åº¦å­¦ä¹ **: åå‘ä¼ æ’­ã€æ¢¯åº¦æ¶ˆå¤±ã€æ‰¹å½’ä¸€åŒ–
3. **ä¼˜åŒ–ç®—æ³•**: SGDã€Adamã€å­¦ä¹ ç‡è°ƒåº¦
4. **è¯„ä¼°æŒ‡æ ‡**: å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1-scoreã€AUC

### å®è·µæŠ€èƒ½
1. **æ•°æ®é¢„å¤„ç†**: ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹ã€ç‰¹å¾ç¼©æ”¾
2. **æ¨¡å‹é€‰æ‹©**: äº¤å‰éªŒè¯ã€è¶…å‚æ•°è°ƒä¼˜ã€æ¨¡å‹é›†æˆ
3. **ç‰¹å¾å·¥ç¨‹**: ç‰¹å¾é€‰æ‹©ã€ç‰¹å¾æ„é€ ã€é™ç»´æŠ€æœ¯
4. **æ¨¡å‹éƒ¨ç½²**: APIè®¾è®¡ã€å®¹å™¨åŒ–ã€ç›‘æ§

### é¡¹ç›®ç»éªŒ
1. **ç«¯åˆ°ç«¯é¡¹ç›®**: ä»æ•°æ®æ”¶é›†åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´æµç¨‹
2. **é—®é¢˜è§£å†³**: å¦‚ä½•å®šä¹‰é—®é¢˜ã€é€‰æ‹©æ¨¡å‹ã€è¯„ä¼°æ•ˆæœ
3. **å·¥ç¨‹å®è·µ**: ä»£ç è§„èŒƒã€ç‰ˆæœ¬æ§åˆ¶ã€æµ‹è¯•ç­–ç•¥
4. **ä¸šåŠ¡ç†è§£**: å¦‚ä½•å°†æŠ€æœ¯ä¸ä¸šåŠ¡ç›®æ ‡ç»“åˆ

## ğŸ“– å­¦ä¹ èµ„æº

### ç»å…¸æ•™æ
1. **ã€Šæœºå™¨å­¦ä¹ ã€‹** - å‘¨å¿—åï¼ˆè¥¿ç“œä¹¦ï¼‰
2. **ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹** - æèˆª
3. **ã€Šæ·±åº¦å­¦ä¹ ã€‹** - Ian Goodfellow
4. **ã€ŠPythonæœºå™¨å­¦ä¹ ã€‹** - Sebastian Raschka

### åœ¨çº¿è¯¾ç¨‹
1. [Andrew Ngæœºå™¨å­¦ä¹ è¯¾ç¨‹](https://www.coursera.org/learn/machine-learning) - å…¥é—¨ç»å…¸
2. [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) - æ·±åº¦å­¦ä¹ 
3. [Fast.ai](https://www.fast.ai/) - å®ç”¨æ·±åº¦å­¦ä¹ 
4. [CS229 Stanford](http://cs229.stanford.edu/) - æ–¯å¦ç¦æœºå™¨å­¦ä¹ 

### å®è·µå¹³å°
1. [Kaggle](https://www.kaggle.com/) - æ•°æ®ç§‘å­¦ç«èµ›
2. [Google Colab](https://colab.research.google.com/) - å…è´¹GPUç¯å¢ƒ
3. [Papers with Code](https://paperswithcode.com/) - è®ºæ–‡ä¸ä»£ç 
4. [Hugging Face](https://huggingface.co/) - NLPæ¨¡å‹åº“

---

é€šè¿‡ç³»ç»Ÿå­¦ä¹ AI/MLæŠ€æœ¯ï¼Œæ‚¨å°†å…·å¤‡è§£å†³å®é™…é—®é¢˜çš„èƒ½åŠ›ï¼Œä¸ºAIç›¸å…³å²—ä½é¢è¯•å’ŒèŒä¸šå‘å±•åšå¥½å‡†å¤‡ã€‚ 